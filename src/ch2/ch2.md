# Chapter 2: the process
### Technologies, specs and sketches

In my second month as intern, I went on implementing the initial sketches that served mainly the purpose of better explaining the ideas I had for the new version of the CAM application.

During the last month, the main focus of CAM was better defined: while it is mostly used as demo for potential clients at the moment, the idea is to migrate to a fully fledged app, targeting those human agents or indexers who are hourly paid to verify and review automatically assigned tags, bringing on the table their own expertise on a topic. For instance, a surgeon gets paid to delete wrong tags, add missing ones and review the general correctness of the analysis of all the tags related to surgery or body anatomy. The goal becomes then to help such a kind of users to do their job in as little time as possible; the challenge is to demonstrate to potential clients that, through the use of the tools this application provides, human indexers can analyze many more documents in a given time span, than they would by manually reading and annotating the text.

Thus, it was decided to proceed developing the first two screens. Since I am working alone on this project, it was decided to slightly change the way we iterate over the screens. First of all, after having roughly sketched the global flow of the application, I should focus on one screen at a time, completing and validating as many features as possible, before skipping to the next section. In addition to this, since no other developer is yet involved, we decided to reduce the sketching phase, with the goal of producing semi-working mock ups which are easier to discuss over for non-technical people. I then adopted the following process:
mock up in the browser a rough representation of a screen
meet together with the Product Manager and the Sales Manager in order to define features and content
iterate until the screen is finished
skip to the next screen

Choosing the framework

The first thing to do at this point, was deciding which framework to adopt so that the development of the functionalities, that enable the application to offer an interactive experience to the user, will be as fast and productive as possible. Naturally, an MVC pattern (or similar) needed to be adopted, as really often happens in web applications and UI development. 
I decided to go for Angular, a JavaScript framework targeting single page applications, after having considered Backbone and React, as they are the main players in the front-end frameworks game. While Backbone offers a much more lightweight JavaScript file and great extensibility and freedom in the choice of plugins, template engine, and so on, this comes at the cost of having to manually write a lot of code and browsing through sparse documentation (each plugin has its own). On the other hand React, the new kid in town, developed by the Facebook’s team and adopted by Instagram.com, is still in its earliest years, meaning there’s not that big of a community yet, plus it somehow reinvents the way web applications are developed nowadays, requiring a little bit of effort by people who come from the already-established way of doing things for the web.
So why Angular? There are a lot of reasons why the decision fell on this framework: first of all, it’s really quick to get going and easy to learn. My first concern in choosing the right environment, though, was going for solutions that will be easily maintained by the company when I’ll finish my stage; people at Mondeca in fact are prevalently trained in Java programming, and they have in fact always developed web UIs through the use of the Google Web Toolkit, which allows them to cross-compile Java code into Javascript and HTML/CSS. Thus, I should make it as straightforward as possible for them to go in and modify something when needed. Plus, a big factor was the availability for Mondeca of an outsourcing partner which already takes care of some of the web UI development for them, and which is a huge expert in Angular-based applications. Therefore, the company can rely on a trusted partner in the future in order to handle my project’s evolution.
However, Angular has a lot of bonus features that are great for CAM: first of all, it offers out-of-the-box two-ways data binding, which is the automatic synchronization of data between the model and view components. The way that Angular implements data-binding lets you treat the model as the single-source-of-truth in the application. The view is a projection of the model at all times. When the model changes, the view reflects the change, and vice versa. This is of course great in many scenarios, but especially for CAM, which is an application that, at its core, takes as input a series of tags, lets the user browse through it and delete some of the tags, then sends as output the outcome of the user’s interactions. In other words, the view needs to simply reflect what’s in the model at all times and each user’s decision can be simply reflected in the model by deleting (or adding) tags to the list. 
In addition to this, Angular relies on a declarative way of binding actions and data to HTML tags, which makes it great for fast and mock-up-centered iterations, where the design phase is limited and there’s the need for a semi-working mock-up as early as possible in order to let the Product and Sales Managers be involved in the process. In fact, since I work alone on the project, I primarily talk with these people, who are neither designers nor developers, so it’s really important that I allow them to focus on the functionality of the application, getting rid of static representations of what the final product will look like.
A final consideration on the drawbacks of using Angular needs to be made: it is a moderately heavy framework that could suffer from a performance perspective when thousands of bindings are present at the same time; this is not extremely important for the typical user that’s being targeted by CAM: indeed, the user is supposed to be on a laptop (no problems related to low-power mobile devices) and it will certainly be willing to wait for a small initial loading time.

Screens: Overview

The first screen I focused on is the “overview“ screen, which is the landing page the user is redirected to after the analysis of a document has completed. The goal is to provide the user a quick glimpse of the most important results of the text mining and content enrichment process, while showing measures that could give some hints about what could be a good course of action from that moment on; so for instance, an administrator could look at this screen and be immediately aware of how good the underlying taxonomy is, whereas a human agent can see if there’s something obvious to do, like wrong automatic deletions of tags, or irrelevant terms that shouldn’t appear in the most relevant terms list. 
Selecting the right quantity and type of information to display in this screen was tricky, since there was the need of containing everything in a single screen (no scroll allowed) while providing meaningful insights to both administrators (who primarily care about taxonomy’s quality and annotating system’s performance) and human indexers (who would like to be helped in the process of understanding what happened during the analysis and what can be easily fixed). For these reasons, my design includes just 9 “widgets”, organized in 3 rows, so that every piece of information is effortlessly reachable by the eye, being it widely separated by blank space from the other elements of the page. Measures and, more generally, data about the document, are grouped in a logical manner: on the left-hand side, “wordy” descriptive information is presented, such as metadata on the document itself, number of words analyzed, configuration settings applied to the pipeline in order to process this resource (mainly a list of the taxonomies involved and whether or not the content classifier was enabled). On the right-hand side, the real dashboard-like content, laid out top to bottom from most informative to most general. As we can see in the screenshot, in the first row I decided to put information that is widely related to the aboutness of the document itself, being it a high-level dissection of the main topics it touches, plus a sequence of tags produced by the underlying classifier, which is capable of spotting existing relationships between elements that come from the global knowledge, which it is formed over the document’s content: what this means is that, basing on some rules configured by the customer, the system can classify the document putting together both what it has been found in the text, and what it has been inferred by means of the taxonomy.

![Overview screen, version A][overview_screen_A]

So, in the example in the picture, the classifier was able to determine that a given article can be identified as “Not for kids” basing on the presence of “adult” terminology, content or topic. 
What it’s interesting here, is that the Web API doesn’t - yet - provide the list of “main topics”, as I called them in my mock-up; the idea of showing this incredibly useful and insightful piece of information came from a discussion that I had with the Sales Manager and one of the core back-end developers of the team. This developer was briefly explaining an algorithm he personally invented to score entities found in the text, by means of clusters. What we realized is that, since the fundamental nature of the model they’re using is tree-like, meaning there’s always available a broader-narrower relationship between resources that allows to bring the model down to a tree, this algorithm can always represent entities detected in the text and their relationships in a way that allows to split such a tree in clusters. By clustering together different entities under a cluster’s root node, the system will easily output relevancy scores for the given terms; thus, in a document presenting a big cluster having as root node the “health” term, and a small cluster having as root “finance”, a term belonging the first cluster will be considered as “more relevant” than a term belonging to the second cluster, by weighing more occurrences of the former than occurrences of the latter. The really interesting notion here is this clustering process, that seems to perform very well and which gives out some interesting analysis of the content of the resource being analyzed; however, clusters are just a step of the whole algorithm, and they are not stored anywhere, nor kept in memory after scoring completes. What my design is suggesting is to revisit the already existing software so that this clusters can be sent to the client application. I strongly believe that this is really useful for CAM, especially when we are thinking of a dashboard, aiming to give the most concise summary of what the annotation process has done, with respect to what’s actually present in the text.
What follows in the screen is somehow more “standard” statistics: in the second row, I put the total number of words analyzed, which is important in order to correctly interpret what follows, a breakdown of all the tags by type, or category, answering to questions like “how many of the tags that we have are coming from the text?”, “how many of those have been inferred by the system?” and so on. Finally, the top 5 most relevant terms, organized in a bar chart.
The third and last row contains measures that are not less important, yet they are placed in the bottom area of the screen, since they are somehow less related to the document itself, but rather more to the quality of the system and of the model. In fact, here the user can find a timing estimate of how long the process was, a breakdown by taxonomies that were involved in the pre configured workflow (which complements the lists already shown in the sidebar, since it’s very likely that not all those taxonomies actually yielded out some term) and the top 5 most frequent classes. In such a way, administrators or taxonomists can visualize which taxonomy is useful and which is not, which classes are always triggered and which ones are probably not that important in the business model of their company.
In conclusion, there is another important thing to notice about this first Overview screen: the whole aboutness row relies on the presence of two plugins, the cluster-based scoring plugin and the rule-based classification plugin. But, as things are now, it is entirely up to the customer to configure those plugins and drop them into their pipeline. What this implies for the design of the screen is that we could not have the first row, since the data it depends on could not be available. The solution for this was designing a “plan B”, introducing a new row, that shows three highly correlated charts: the most relevant terms, as it is already shown in the original version, the most frequent terms, which is basically the same chart ordered by occurrences, and the most frequent AND relevant terms, which is a combination of both the previous charts. These three graphics, when compared with one another, tell the user much a deeper story about the terms that are found in the text, thus forming a great fallback in absence of the “aboutness” data.

![Overview screen, version B][overview_screen_B]

Screens: Review

Regarding the “review” screen, its main goal is to enable the users to quickly browse through a possibly very long list of tags, understand the logic that made the system annotate the document with those tags and, in case she considers some annotation wrong, remove one or more particular tags. Moreover, whenever the user realizes that an important tag is missing, either because it wasn’t detected in the text or because she thinks it’s correlated and useful to the company owning the content, she can add it: let’s say, an article covering some aspect of Michael Schumacher’s life, but never mentioning F1, can still be annotated with a “F1” tag by a human agent, in case this couldn’t be automatically inferred by the system. With these objectives in mind I approached the design of this screen from the user’s point of view entirely. In order to better express how I did this, let’s rely on a user story: 

“a human agent expert in some given area is provided with a list of already annotated documents to verify and review, being paid basing on the throughput of documents”

Having a user story, no matter how brief and straightforward it may be, is powerful way to stay focused on what the user’s needs and goals are; in fact, since I knew from the beginning what the final experience of my targets should be, I could concentrate on what really matters to them. My idea is to get rid of every static representation of the outcome of the analysis, and to allow people to narrow down the amount of information they are presented with letting them focusing on the possibly few annotations they are interested in. In order to achieve this kind of user experience, I provide two simple yet powerful tools: a table of tags and some filters. By using the filters, a human indexer can easily see only the tags she is supposed to review and get her job done in the smallest amount of time possible; moreover, by making good use of the high-level information she has just seen in the Overview screen, quick action can be immediately taken, without having to spend too much time digging into the text, details, comparisons, and so on. At this point, I was more than ever decided to go for a single-page web application: it doesn’t feel right to have to refresh the page every time the user hits on a filter, as it usually happens in the competitor's’ tools; for this reason I feel like adopting a heavy client-side framework like Angular was the right call. 
The third most important element of a UI of this kind is, of course, the original text, and the context in which detected terms are found: in other word, it is important to have access to the position in the text where an entity lives, so to not only be able to recognize whether or not the annotation was correct, but also to understand why an element appears on the list (it happened in the past of not understanding why there was a “bear” in the list, because the system simply normalized the past participle “born” into “bear”; in such cases, it’s extremely important to be able to track back to the analyzed text). In order to address this, I decided to split the screen into two equally sized columns, one containing the table of tags, the other containing the annotated text, while hiding behind a toggle button a drawer panel containing the filters. A further adjustment to the design that I made was not overlaying the panel to the two columns, rather “pushing” the content to the right, basically displaying simultaneously on the page three columns: filters, tags and text. This was done with the goal in mind of allowing the user to see what’s changing in the list and in the annotated document, as she filters out entries; and this is made especially important by the interactive nature of the application I set out to build, since it would be useless to have a single-page web application, capable of instantaneously react to user’s input, if the filters’ bar covered partially the resulting outcome of those actions.

![Review screen, filters open][review_screen_open]

Finally, always along the lines of helping a human agent do her job quickly and promptly, I added breadcrumb faceted navigation on top of the screen, to help the user keep track of what filters are enabled, at any given time. This may seem a strange decision, since there’s no actual hierarchy in the filtering process, but it is still useful to have it as a feature in the app, since we don’t want CAM’s users to validate tags just because they forgot that something can be out of the list that’s on the screen in that moment. This also gave me a chance to put a “reset filters” button in a place where it could be intuitive for the user to find it.

![Review screen, breadcrumb faceted navigation][review_screen_breadcrumb]

A UCD-based approach

There are a couple of aspects of the review screen that certainly demonstrate how designing for the user, or User Centered Design (UCD), is a great approach to make more usable products. In fact, some small additions and modification to the global behaviour of CAM are entirely due to this way of working, like the “reset filters” button I already mentioned, which enhances the user experience by reducing the number of clicks to switch from one filter to the other by a great deal, adopting a single color to highlight the terms in the text instead of using a color per class or category, which greatly improves the readability of the text, or providing affordances for the user to undo an action or rollback to a previous state, for example when she deletes some tags.

![Review screen, removing notification, with possibility to undo][review_screen_notification]

The greatest challenge I’m facing designing the new version of CAM is being able of both improving the existing demo and suggest new features to be integrated in the application, so that it could be used as a stand-alone tool in the future. While doing this, I had the chance to modify some aspects of the user’s flow, revisiting the experience. A good example of this is the “add tag” action, which used to consist in the steps:
select Class
select Label among the ones corresponding to the selected class
This probably makes a lot of sense to a developer who also knows how the taxonomy works, but it doesn’t to a human indexer who usually isn’t trained in those terms. What actually happens during the usage of this function is:
the user thinks of a tag she wants to add
the user tries to help taxonomists by indicating a class
I made the process more user friendly, by guiding through the tag addition with a wizard, giving also the possibility not to suggest any class, since one could know what to do with the tag she is thinking of. Also, I got rid of the notion of “class” by using the “type” term, which can be less confusing for some users. In addition to this, I improved the way new tags get integrated in the UI, by adding a “manual entry” type, that can be reached through the filters. As a result, the user has a quick method to revisit what she added, and possibly change her mind.

