# Perfmatters: a deep dive into performance

When it comes to performance on the web, more and more often developers don’t take into account the external factors that constrain an application's speed, which don’t depend on architectural patterns or optimized data structures. However, those are the real bottlenecks in the web of today, especially when targeting a desktop environment, where developers can count on medium to high processing power. I believe this is a really important problem to address for CAM, and other Mondeca's products as well, since we are not producing applications to be used on mobile; in addition, most of the jobs and features that such products offer are run on servers, therefore the client needs to continuously rely on network requests and gets slowed down by the decently big amount of data that needs to be transferred on the wire. For these reasons, it feels compelling to reduce the impact on performance of elements that are independent from the execution of the jobs themselves.
Most commonly, such elements are the following:

 - _file size_, which basically determines the amount of time spent by the browser on downloading the assets needed to display the requested resource; this can be greatly reduced by means of established techniques such as minification and compression of assets
 - _number of HTTP requests_, which greatly affects the bootstrap of a web page since every HTTP connection that the client needs to open adds a fairly significant overhead, which is generally big compared to the time the client will actually spend downloading the resource, especially when such a resource has been minified and compressed; it exists a simple and powerful solution to this problems, which consists in concatenating as many resources as possible in a single file. This technique, when combined with the previous ones that aim at reducing the file size, can optimize the loading time by a great factor

By means of concatenation, minification and compression techniques, the page can load faster and feel instantaneous to the user. However, other things can be done in order to further improve the experience for the user; first of all, though, it’s important to have valid measurements in order to understand what needs to be improved, take action and then validate the results. 
Thus, I performed a performance audit on CAM, tested some solutions and then evaluated the effects against some known indexes and thresholds.

## Two metrics: loading speed

As the current state of the application is at its earliest stages, I mainly focused on two “metrics” for performance gage: loading speed and runtime smoothness. Please also note that, at the time these measurements were taken, no actual server-side processing was taking place, and the actual data shown by the application was mocked-up and dished out as a JSON file to the requesting client. Even though this may seem to be invalidating the measurements, being that I am referring to a situation that it’s not going to be replicated in real use case scenarios, it is actually meaningful to try and optimize the performance of the application at this point in time, since my goal, as a front-end developer, is to reduce the impact of the application shell on the global performance as much as possible, possibly shooting at adding an overhead to the whole process of indexing documents that can be considered as negligible.

Let’s start with loading speed. The most common metric referred to when assessing loading speed is the so called speed index, which basically is the average time at which visible parts of the page are displayed, expressed in milliseconds. As the _webpagetest.org_ documentation puts it (emphasis added) 

>"The Speed Index metric was added to WebPagetest in April, 2012 and measures **how quickly the page contents are visually populated** (where lower numbers are better).  It is particularly useful for comparing experiences of pages against each other (before/after optimizing, my site vs competitor, etc) and should be used in combination with the other metrics (load time, start render, etc) to better understand a site's performance."

Webpagetest.org is one of the most popular and useful online tools for performance evaluation on the web; it basically runs a tests suite over a given URL and outputs a series of tips to improve the performance and important measures to be aware of. For the sake of this first experiment, I am only reporting the Speed Index, which was __4575__. This figure doesn’t tell a lot per se, but it is important to know that a solid speed index value is around 1000, as it will be covered in the next sections.

In order to dig deeper into what could be causing a lower loading speed than expected, I took a look at both the network usage and the JavaScript profile, thanks to the Chrome DevTools. The network traffic analysis revealed that the client was spending too much time downloading the resources, and this was slowing down the first paint of the page; thus, the first improvement I made was relying on the established techniques explained in the previous paragraphs: indeed, by means of concatenation, minification and compression techniques the time to first paint dropped down from roughly 1 second to 0.5 seconds, on my testing environment. The reason why this happens lies in the way the browser’s parser is dealing with external resources when first analyzing (and then rendering) a page: it goes from top to bottom through the HTML file, it fetches the resource from the server every time it encounters a script tag or style link, and only then, the first paint appearing on screen can take place. It needs to be acknowledged that the speed index is taking into account the time at which the page is __mostly visually complete__, whereas the time to first paint it’s just the moment at which the browser can actually __start rendering__ pixels on the screen. There is an important difference between the two metrics, which can be brought down to a single fundamental concept: perceived performance.

## An orthogonal metric: perceived performance

At the same way UCD predicates designing for the user, we shall put the user at the center while improving performance. Opposed to computational performance, perceived performance refers to how quickly a process appears to be completed to the user, which may often differ from the actual speed that process has been completed at. For instance, the amount of time an application takes to start up, or a file to download, is not made any faster by showing a splash screen or a progress bar. However, it satisfies some human needs: it appears faster to the user as well as providing a visual cue to let them know the system is handling their request. This is a very relied upon technique when trying to deliver a great experience of use for a not-so-snappy system; when it comes to web applications, the goal is to be able to display parts or previews of the final status the screen will reach, while loading it. Therefore, coming back to difference between speed index and time to first paint, the latter doesn’t give an accurate indication of how responsive the application will be __perceived__.  In this context, recently it has been gaining a lot of popularity a new technique consisting in showing the application shell as quickly as possible, then dynamically injecting content into the page. In order to achieve this, I had to first solve another issue that was heavily affecting the pages load time, causing that 4000+ speed index to be so high; that is, an Angular project is typically composed of a lot of HTML templates, in order to enhance code readability and maintainability. However, the downside of such a module-based approach, is that every directive or template included in the page is going to trigger an HTTP GET to the server requesting for the corresponding HTML file. So how do we solve this problem? I tested three solutions, basically building every new idea on top of the previous one.

At first, I enabled templates __caching__ through use of Angular’s `$templateCache` service. This allows Angular to fetch every template only once, and reuse it throughout the whole application every time it is requested again. While such a strategy improves the overall navigation experience, it still doesn’t solve the problem of having to contact the server the first time an asset is needed. Then, I instructed the application to __prefetch__ all the HTML assets after all the core requests have been handled and the first page has been completely loaded. The objective of such a strategy is to prepare the cache with all the assets that will be needed in other sections or pages, even they may be hidden at first. 
While this two improvements, when applied together, greatly reduce the delays introduced by the need of continuously fetching the HTML templates, they don’t improve the perceived speed of the application, rather they deal with the computational one. The user was still looking at a blank page until all the resources were ready: indeed, such caching and prefetching strategies rely on Angular to be fully loaded and functioning. Therefore, I decided to __inline__ all the templates into a single, minified, JavaScript file (under the form of simple strings) and to include such a resource into the index.html. But why is this any better than the previous solutions? Here's what happens in the browser in the two situations:

Without inlining:

 - the browser fetches the index.html 
 - the browser fetches the script.min.js, which is the file containing all the application's code
 - the browser runs such script, which will then fetch the missing templates from the server
 - while the network request is still pending, the browser goes on parsing and rendering the first page, asking __to the server__ every HTML template it needs for the first load

With inlining:

 - the browser fetches the index.html
 - the browser fetches the script.min.js
 - the browser fetches a brand new templates.min.js file, containing inline declarations of all the templates used throughout the application
 - the browser goes on parsing and rendering the first page, asking __to the cache__ every HTML template it needs for the first load

Even though the difference may not seem that big, it has great implications on the perceived loading speed. Indeed, the user is now almost immediately shown the application shell, meaning that the whole structure of the application (header, navigation bar, sidebar, and so on) is almost immediately rendered, while the dynamically injected content gets displayed as soon as it is available. However, in order to better define the viability of this approach, one must define what "almost immediately" means, and understand why adding one HTTP request at the beginning of the page loading process (that template.min.js additional resource) feels faster to the user, than it does fetching an HTML template when needed. The "application shell" strategy is backed up by several studies, and has its roots in the publication _Response Times: The 3 Important Limits_ by Jakob Nielsen on January 1, 1993.
Thanks to this study, we can now quantify qualitative measurements such as "immediate" and "instantaneous": indeed, Nielsen proved that a waiting time under 100ms feels instantaneous, while a waiting time around 1000ms signifies a context switch to the user. Further analysis have further expanded these concepts and, most recently, have led to the definition of the RAIL model, which is defined as by the Chrome team who came up with this new concept as (emphasis mine):

>"a **user-centric performance model**. Every web app has these four distinct aspects to its life cycle, and performance fits into them in very different ways: Response, Animation, Idle and Load"

Basically, the RAIL model gives us developers a few figures that we can refer to whenever we are facing challenges in improving the perceived performance; indeed, thanks to RAIL, we are finally left with the following timing constraints:

 - 16ms per frame during an animation
 - 100ms of response time, upon user's interaction
 - 1000ms of loading time

But let's bring this back to CAM. This model explains why the inlining strategy is so successful: during load time, we're working with a 1000ms time span, during which the browser can fetch the additional templates.min.js file. It’s perfectly fine for the user to be waiting up to 1 second for the application to load, thus one can make use of this and load the templates resource; as a consequence, two milestones are achieved:

 - the user sees the application shell all at once, in under a second time, and mentally establishes that the loading phase is done (even if it’s not);
 - when the user will change views or pages, all the structural element will be immediately in place, giving the perception of instantaneous navigation

Here’s what the pages look like before the content is dynamically injected.

![Overview screen, application shell][overview-appshell]

![Review screen, application shell][review-appshell]

## Two metrics: runtime smoothness

Regarding the runtime smoothness of the application, a few tweaks needed to be done. First of all, the meaningful metric here is the frames per seconds (FPS), the goal is to ensure that animations and scrolling play out at 60fps, and again I relied on the Chrome DevTools in order to reliably get this information. Since the “analyze” screen is basically mostly static, I focused on the “review” and “overview” screen; both of them used to have some performance issues in terms of animation smoothness, but the origin of such low fps turned out to be very different in the two cases.
In the “overview” screen, a visible lag could be seen while first loading the page, and this feeling was confirmed by the depressing value of 3fps in some steps; indeed, the charts are animated (bars grow to their final size, pies build up in a fancy way) as soon as the page is loaded. A deep look into the Timeline view in the tools revealed what was causing it: the Angular’s directive I am using to stamp out those charts (which is basically wrapping the Highcharts.js API) was running its underlying JavaScript in order to make the animation happen. Regardless of whether or not this might be the right way to do it (more often than not, such JavaScript-empowered transitions do not rely on GPU acceleration, even though I can’t be sure since I didn’t dig into the implementation code of it), this was causing the browser to spend a lot of time on every single frame, while, since we are aiming to rendering at 60fps, we are left with less than 16ms per frame; moreover, by further analyzing the timeline, we can see 8 “spikes”, which bring to the surface another problem: the directives are running too many times, in fact one can see 8 “charts” loading and drawing, while only 4 are being display on screen. Why is this happening?

![Overview screen, JavaScript profile in the Timeline][overview-jsprofile]

This page was built with two versions in mind, one that should be displayed when the cluster-based scoring plugin is available, and one when it’s not. In order to achieve this effect without duplicating too much code, I was simply hiding and showing widgets as needed, programmatically. However, for how Angular’s directives work, the hiding/showing action takes place after the basic directive functionalities are loaded (needed DOM is linked to the root element, controller’s code has run, and so on). Therefore, exploiting the fact that every HTML template I’d need is going to be precached at load time anyway, I decided to split such a page into its two versions; by doing this, I solved the performance issue, while making the code much more readable and maintainable. It’s also worth noting that in Angular it can be possible to lazy load the directives, which would have solved the issue as well, but I decided nonetheless to take the “maintainability path”.

Much a different process was fixing the performance issues in the review screen. While the previous one had a lot to do with the JavaScript being run, this one finds its causes in the way CSS properties are transitioned by the browser’s rendering engine. First of all, a little bit of context: in the review screen, the transition between filters bar closed and filters bar open states is animated, provoking the subsequent reduction of both the width and the font-size of the tags’ table and document container. As it turns out, this can be cumbersome to render at 60fps with the current technologies, for the way the engine takes care of those properties change; for the sake of this report, what matters is that maximum performance can be obtained only when tinkering with transform and opacity, which, as it currently stands, are the only properties that don’t trigger neither the layout nor the painting steps of the rendering process. 

![http://csstriggers.com/ - It offers a quick  summary of which CSS properties trigger which phase of the rendering process][csstriggers]

Therefore, one might think that the fix would simply consist in animating the transform property of the filters, in order to exploit GPU acceleration; however, this was already the case, as the filters’ box was translating into view by means of transform: `translateX(0)`. What was missing was a hint to the browser, telling it that the HTML element containing the filters’ box was going to change its transform property, thus allowing it to be layer-promoted. Without going too much into the technicalities of this procedure, whenever a moving element needs to be computed by the compositor thread, it also needs to live within its own layer; in the past, in order to place an element inside a new layer, there was the need to use the transform: `translateZ(0)` hack, which was basically forcing layer-promotion. Today however, it suffices to use will-change: transform, which enables every kind of optimization for that kind of transition by the rendering engine, layer-promotion included.

