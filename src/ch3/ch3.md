# Chapter 3: the core screens
### Overview and review

Screens: Overview

The first screen I focused on is the “overview“ screen, which is the landing page the user is redirected to after the analysis of a document has completed. The goal is to provide the user a quick glimpse of the most important results of the text mining and content enrichment process, while showing measures that could give some hints about what could be a good course of action from that moment on; so for instance, an administrator could look at this screen and be immediately aware of how good the underlying taxonomy is, whereas a human agent can see if there’s something obvious to do, like wrong automatic deletions of tags, or irrelevant terms that shouldn’t appear in the most relevant terms list. 
Selecting the right quantity and type of information to display in this screen was tricky, since there was the need of containing everything in a single screen (no scroll allowed) while providing meaningful insights to both administrators (who primarily care about taxonomy’s quality and annotating system’s performance) and human indexers (who would like to be helped in the process of understanding what happened during the analysis and what can be easily fixed). For these reasons, my design includes just 9 “widgets”, organized in 3 rows, so that every piece of information is effortlessly reachable by the eye, being it widely separated by blank space from the other elements of the page. Measures and, more generally, data about the document, are grouped in a logical manner: on the left-hand side, “wordy” descriptive information is presented, such as metadata on the document itself, number of words analyzed, configuration settings applied to the pipeline in order to process this resource (mainly a list of the taxonomies involved and whether or not the content classifier was enabled). On the right-hand side, the real dashboard-like content, laid out top to bottom from most informative to most general. As we can see in the screenshot, in the first row I decided to put information that is widely related to the aboutness of the document itself, being it a high-level dissection of the main topics it touches, plus a sequence of tags produced by the underlying classifier, which is capable of spotting existing relationships between elements that come from the global knowledge, which it is formed over the document’s content: what this means is that, basing on some rules configured by the customer, the system can classify the document putting together both what it has been found in the text, and what it has been inferred by means of the taxonomy.

![Overview screen, version A][overview_screen_A]

So, in the example in the picture, the classifier was able to determine that a given article can be identified as “Not for kids” basing on the presence of “adult” terminology, content or topic. 
What it’s interesting here, is that the Web API doesn’t - yet - provide the list of “main topics”, as I called them in my mock-up; the idea of showing this incredibly useful and insightful piece of information came from a discussion that I had with the Sales Manager and one of the core back-end developers of the team. This developer was briefly explaining an algorithm he personally invented to score entities found in the text, by means of clusters. What we realized is that, since the fundamental nature of the model they’re using is tree-like, meaning there’s always available a broader-narrower relationship between resources that allows to bring the model down to a tree, this algorithm can always represent entities detected in the text and their relationships in a way that allows to split such a tree in clusters. By clustering together different entities under a cluster’s root node, the system will easily output relevancy scores for the given terms; thus, in a document presenting a big cluster having as root node the “health” term, and a small cluster having as root “finance”, a term belonging the first cluster will be considered as “more relevant” than a term belonging to the second cluster, by weighing more occurrences of the former than occurrences of the latter. The really interesting notion here is this clustering process, that seems to perform very well and which gives out some interesting analysis of the content of the resource being analyzed; however, clusters are just a step of the whole algorithm, and they are not stored anywhere, nor kept in memory after scoring completes. What my design is suggesting is to revisit the already existing software so that this clusters can be sent to the client application. I strongly believe that this is really useful for CAM, especially when we are thinking of a dashboard, aiming to give the most concise summary of what the annotation process has done, with respect to what’s actually present in the text.
What follows in the screen is somehow more “standard” statistics: in the second row, I put the total number of words analyzed, which is important in order to correctly interpret what follows, a breakdown of all the tags by type, or category, answering to questions like “how many of the tags that we have are coming from the text?”, “how many of those have been inferred by the system?” and so on. Finally, the top 5 most relevant terms, organized in a bar chart.
The third and last row contains measures that are not less important, yet they are placed in the bottom area of the screen, since they are somehow less related to the document itself, but rather more to the quality of the system and of the model. In fact, here the user can find a timing estimate of how long the process was, a breakdown by taxonomies that were involved in the pre configured workflow (which complements the lists already shown in the sidebar, since it’s very likely that not all those taxonomies actually yielded out some term) and the top 5 most frequent classes. In such a way, administrators or taxonomists can visualize which taxonomy is useful and which is not, which classes are always triggered and which ones are probably not that important in the business model of their company.
In conclusion, there is another important thing to notice about this first Overview screen: the whole aboutness row relies on the presence of two plugins, the cluster-based scoring plugin and the rule-based classification plugin. But, as things are now, it is entirely up to the customer to configure those plugins and drop them into their pipeline. What this implies for the design of the screen is that we could not have the first row, since the data it depends on could not be available. The solution for this was designing a “plan B”, introducing a new row, that shows three highly correlated charts: the most relevant terms, as it is already shown in the original version, the most frequent terms, which is basically the same chart ordered by occurrences, and the most frequent AND relevant terms, which is a combination of both the previous charts. These three graphics, when compared with one another, tell the user much a deeper story about the terms that are found in the text, thus forming a great fallback in absence of the “aboutness” data.

![Overview screen, version B][overview_screen_B]

Screens: Review

Regarding the “review” screen, its main goal is to enable the users to quickly browse through a possibly very long list of tags, understand the logic that made the system annotate the document with those tags and, in case she considers some annotation wrong, remove one or more particular tags. Moreover, whenever the user realizes that an important tag is missing, either because it wasn’t detected in the text or because she thinks it’s correlated and useful to the company owning the content, she can add it: let’s say, an article covering some aspect of Michael Schumacher’s life, but never mentioning F1, can still be annotated with a “F1” tag by a human agent, in case this couldn’t be automatically inferred by the system. With these objectives in mind I approached the design of this screen from the user’s point of view entirely. In order to better express how I did this, let’s rely on a user story: 

>“a human agent expert in some given area is provided with a list of already annotated documents to verify and review, being paid basing on the throughput of documents”

Having a user story, no matter how brief and straightforward it may be, is powerful way to stay focused on what the user’s needs and goals are; in fact, since I knew from the beginning what the final experience of my targets should be, I could concentrate on what really matters to them. My idea is to get rid of every static representation of the outcome of the analysis, and to allow people to narrow down the amount of information they are presented with letting them focusing on the possibly few annotations they are interested in. In order to achieve this kind of user experience, I provide two simple yet powerful tools: a table of tags and some filters. By using the filters, a human indexer can easily see only the tags she is supposed to review and get her job done in the smallest amount of time possible; moreover, by making good use of the high-level information she has just seen in the Overview screen, quick action can be immediately taken, without having to spend too much time digging into the text, details, comparisons, and so on. At this point, I was more than ever decided to go for a single-page web application: it doesn’t feel right to have to refresh the page every time the user hits on a filter, as it usually happens in the competitor's’ tools; for this reason I feel like adopting a heavy client-side framework like Angular was the right call. 
The third most important element of a UI of this kind is, of course, the original text, and the context in which detected terms are found: in other word, it is important to have access to the position in the text where an entity lives, so to not only be able to recognize whether or not the annotation was correct, but also to understand why an element appears on the list (it happened in the past of not understanding why there was a “bear” in the list, because the system simply normalized the past participle “born” into “bear”; in such cases, it’s extremely important to be able to track back to the analyzed text). In order to address this, I decided to split the screen into two equally sized columns, one containing the table of tags, the other containing the annotated text, while hiding behind a toggle button a drawer panel containing the filters. A further adjustment to the design that I made was not overlaying the panel to the two columns, rather “pushing” the content to the right, basically displaying simultaneously on the page three columns: filters, tags and text. This was done with the goal in mind of allowing the user to see what’s changing in the list and in the annotated document, as she filters out entries; and this is made especially important by the interactive nature of the application I set out to build, since it would be useless to have a single-page web application, capable of instantaneously react to user’s input, if the filters’ bar covered partially the resulting outcome of those actions.

![Review screen, filters open][review_screen_open]

Finally, always along the lines of helping a human agent do her job quickly and promptly, I added breadcrumb faceted navigation on top of the screen, to help the user keep track of what filters are enabled, at any given time. This may seem a strange decision, since there’s no actual hierarchy in the filtering process, but it is still useful to have it as a feature in the app, since we don’t want CAM’s users to validate tags just because they forgot that something can be out of the list that’s on the screen in that moment. This also gave me a chance to put a “reset filters” button in a place where it could be intuitive for the user to find it.

![Review screen, breadcrumb faceted navigation][review_screen_breadcrumb]


Screens: Analyze
Introducing the “analyze” screen (in lack of a better name). This screen is probably going to be the first one that the users are going to interact with: indeed, it contains the core controls which enable them to choose the resources to submit to the analysis process, see the progress of said analysis and then dive into the results for the selected resource. Moreover, here one can change some of the workflow’s parameters: remove some taxonomies from the list of the ones that will be used, disable the classifier or specify which rules should it take into account during its classification process (which is, indeed, rule-based).
In order to design a solution for all of these tasks, I decided to split the needs the user might have in two sections: on one hand there is the resource selection, which should include buttons to add files/resources, a list of the selected ones, a way to change the parameters, and so on; on the other hand, there is the process monitoring, which should provide visual feedback for the user, such as an indication of how long the process will take, which resources have already been processed, and so on. 
After having done this, I tackled the design challenges and, through a sequence of iterations, I came up with a solution composed by the following elements:

A sidebar, on the left-hand side of the page, visually representing the process monitoring section; it includes, from top to bottom:
a button to perform the most important action - running the analysis on all the selected resources
an overview of the work already done and yet to be
the global progress (e.g. - 40% completed)
some statistics on the results of the review of the documents performed by the user itself (e.g. removed tags)
The choice of having a sidebar comes from two considerations: first, it is consistent with the general design of the application, which presents a sidebar in almost every section, and secondly, it makes perfect sense to logically separate the “action”, displaced in the main area of the page, from the “details”.
From a design point of view, grouping is the technique I relied upon the most in order to visually represent the relationship existing among the various pieces of data. In fact, by doing this, it becomes obvious that what’s going to appear and change on the left, is just an addition, or an extra if you will, to what occupies the main portion of the screen, which instead is the most important thing to pay attention to.

![Analyze screen, global glance][analyze_screen]

The main content, visually representing the resource selection section. Here, the most prominent element is the list of selected documents to be indexed; indeed, I decided to represent each resource with a single, well-defined, block, rather than with a simple entry in a table or list. 

This is meant to be a visual signal that every single one of those reveals a big process, involving the content augmentation job performed by the underlying system, the tags review and modification by the human agent, while requiring special attention in the configuration and parameters selection, since every workflow (as it is called in CAM) relates to a specific type of content, to some types of file (if the document is contained in a file, as it often happens with CAM), and it has been configured by carefully choosing the plugins to be activated during the analysis (which, for instance, affect the way relevancy scores are computed). Therefore, every document is spaced-out from the others, has its own status indicator (not processed, ready, reviewed) and its own configuration. 

In this section of the application, there is a lot going on. Thus, my approach consists in hiding as much complexity as possible by providing a single point of contact between client and server. Indeed, the main object the user interacts with is a “pool” of documents to be sent to the server; she can populate such a pool in different ways: adding files from a pre-configured Dropbox folder or the local hard drive, by pointing to web resources through URLs, or even by manually entering text (this functionality hasn’t been developed yet, but it will be). There are many things the user can do, a part from populating the pool, such as setting different workflows to different documents, running or re-running only some of the resources, filtering the list, changing the parameters characterizing the workflows themselves (e.g. which taxonomies to use), and so on, but, by putting the list of “to-dos” that will be sent to server at the center, not only the user doesn’t get overwhelmed by the variety of options and nuances, but - most importantly - she is always under control.

